{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> Understanding Markov Chain Monte Carlo</h2></center>\n",
    "<img src= \"MCMC_image.jpeg\" style=\"width:200px ; height=100px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [What are Stochastic Processes..?](#Sto_pro)\n",
    "\n",
    "* [Introduction To Markov chain](#Introduction)\n",
    "\n",
    "* [One Text Processing Example](#one_example)\n",
    "\n",
    "* [Markov chain vs Bayesian Models](#MC_BM)\n",
    "\n",
    "* [What are Hidden markov Models](#HMM)\n",
    "\n",
    "* [Problems to be solved with HMM](#prob)\n",
    "\n",
    "* [Implementing HMM in python](#impl)\n",
    "\n",
    "* [What is Monte Carlo...!](#MC)\n",
    "\n",
    "* [Resources](#Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Sto_pro\"></a>\n",
    "### What are Stochastic processes..?\n",
    "\n",
    "* A stochastic process means that one has a system for which there are observations at certain times, and that the outcome, that is, the observed value at each time is a random variable.\n",
    "* This means that, at each observation at a certain time, there is a certain probability to get a certain outcome. In general, that probability depends on what has been obtained in the previous observations. The more observations we have made, the better we can predict the outcome at a later time. \n",
    "* However, such a general situation becomes very cumbersome, and is almost hopeless to treat by any manageable formalism. For that reason, one usually tries to keep to simplified processes, still quite relevant.\n",
    "* A Markov process is a process where all information that is used for predictions about the outcome at some time is given by one, latest observation(not by all previous observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Introduction\"> </a>\n",
    "### Introduction To Markov Chain\n",
    "\n",
    "* A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules\n",
    "* The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed\n",
    "* Markov Chains are usually modeled by FSMs (Finite state machines)\n",
    "* Markov chains have MARKOV PROPERTY at their core, \"(the probability of) future actions are not dependent upon the steps that led up to the present state\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"one_example\"></a>\n",
    "### Text Processing Example\n",
    "\n",
    "* Use a Markov chain to create a statistical model of a piece of English text. Simulate the Markov chain to generate stylized pseudo-random text.\n",
    "*  Shannon approximated the statistical structure of a piece of text using a simple mathematical model known as a Markov model. A Markov model of order 0 predicts that each letter in the alphabet occurs with a fixed probability. We can fit a Markov model of order 0 to a specific piece of text by counting the number of occurrences of each letter in that text, and using these frequencies as probabilities. For example, if the input text is \"gagggagaggcgagaaa\", the Markov model of order 0 predicts that each letter is 'a' with probability 7/17, 'c' with probability 1/17, and 'g' with probability 9/17 because these are the fraction of times each letter occurs. The following sequence of letters is a typical example generated from this model:\n",
    "\n",
    "            g a g g c g a g a a g a g a a g a a a g a g a g a g a a a g a g a a g ... \n",
    "* A Markov model of order 0 assumes that each letter is chosen independently. This independence does not coincide with statistical properties of English text because there a high correlation among successive letters in a word or sentence. For example, 'w' is more likely to be followed with 'e' than with 'u', while 'q' is more likely to be followed with 'u' than with 'e'.\n",
    "\n",
    "* We obtain a more refined model by allowing the probability of choosing each successive letter to depend on the preceding letter or letters. A Markov model of order k predicts that each letter occurs with a fixed probability, but that probability can depend on the previous k consecutive letters. Let a k-gram mean any k consecutive letters. Then for example, if the text has 100 occurrences of \"th\", with 60 occurrences of \"the\", 25 occurrences of \"thi\", 10 occurrences of \"tha\", and 5 occurrences of \"tho\", the Markov model of order 2 predicts that the next letter following the 2-gram \"th\" is 'e' with probability 3/5, 'i' with probability 1/4, 'a' with probability 1/10, and 'o' with probability 1/20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"MC_BM\"></a>\n",
    "### Markov Chain vs Bayesian Model\n",
    "\n",
    "Before directly jumping to see the difference, let's understand what the hell PGM is...!\n",
    "<u><b><h4>Probabilistic graphical model: </h4></b></u> \n",
    "\n",
    "* It is a graph formalism for compactly modeling joint probability distributions and dependence structures over a set of random variables\n",
    "\n",
    "Ok, but what it has to do with MC and Bayesian model...?\n",
    "\n",
    "* A PGM is called a Bayesian network when the underlying graph is directed, and a Markov network/Markov random field when the underlying graph is undirected (which has little to do with a Markov process despite \"Markov\" in the name). Simply speaking, you use the former to model probabilistic influence between variables that have clear directionality, otherwise you use the latter.\n",
    "\n",
    "* The simplest Markov Process, is discrete and finite space, and discrete time Markov Chain. You can visualize it as a set of nodes, with directed edges between them. The graph may have cycles, and even loops. On each edge you can write a number between 0 and 1, in such a manner, that for each node numbers on edges outgoing from that node sum to 1\n",
    "\n",
    "* The main weakness of Markov networks is their inability to represent induced and non-transitive dependencies; two independent variables will be directly connected by an edge, merely because some other variable depends on both. As a result, many useful independencies go unrepresented in the network. To overcome this deficiency, Bayesian networks use the richer language of directed graphs, where the directions of the arrows permit us to distinguish genuine dependencies from spurious dependencies induced by hypothetical observations.\n",
    "\n",
    "* Must read: state-space diagram:\n",
    "    \n",
    "    https://stats.stackexchange.com/questions/100047/difference-between-bayesian-networks-and-markov-process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"HMM\"></a>\n",
    "### \"Hidden\" Markov Models\n",
    "\n",
    "* The Hidden Markov Model is a finite set of states, each of which is associated with a (generally multidimensional) probability distribution []. Transitions among the states are governed by a set of probabilities called transition probabilities. In a particular state an outcome or observation can be generated, according to the associated probability distribution. It is only the outcome, not the state visible to an external observer and therefore states are <b><u>\"hidden\"</u></b> to the outside; hence the name Hidden Markov Model\n",
    "* A hidden Markov model (HMM) is one in which you observe a sequence of emissions, but do not know the sequence of states the model went through to generate the emissions. Analyses of hidden Markov models seek to recover the sequence of states from the observed data.\n",
    "\n",
    "*  The Hidden Markov Model (HMM) is a variant of a finite state machine having a set of hidden states, Q, an output alphabet (observations), O, transition probabilities, A, output (emission) probabilities, B, and initial state probabilities, Π. The current state is not observable. Instead, each state produces an output with a certain probability (B). Usually the states, Q, and outputs, O, are understood, so an HMM is said to be a triple, ( A, B, Π ).\n",
    "\n",
    "* Must Read:\n",
    "\n",
    "    https://drive.google.com/viewerng/viewer?url=https://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf\n",
    "    \n",
    "    https://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prob\"> </a>\n",
    "### Problems to be solved with HMM\n",
    "Now with the HMM what are some key problems to solve?\n",
    "* Problem 1, Given a known model what is the likelihood of sequence O happening?\n",
    "* Problem 2, Given a known model and sequence O, what is the optimal hidden state sequence? This will be useful if we want to know if the weather is “Rainy” or “Sunny”\n",
    "* Problem 3, Given sequence O and number of hidden states, what is the optimal model which maximizes the probability of O?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"impl\"> </a>\n",
    "### Implementing HMM in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "states= (\"Rainy\",\"Sunny\")\n",
    "observations=('walk', 'shop', 'clean')\n",
    "\n",
    "start_probability= {'Rainy':0.6, 'Sunny':0.4}\n",
    "transition_probability= {\n",
    "    'Rainy' : {'Rainy': 0.7,'Sunny':0.3},\n",
    "    'Sunny' : {'Rainy': 0.4,'Sunny':0.4},\n",
    "}\n",
    "\n",
    "emission_probability={\n",
    "    'Rainy': {'walk':0.1 ,'shop':0.4 ,'clean':0.5},\n",
    "    'Sunny': {'walk':0.6 ,'shop':0.3 ,'clean':0.1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_= np.array([0.6, 0.4])\n",
    "model.transmat_= np.array([[0.7,0.3],\n",
    "                           [0.4,0.6]])\n",
    "model.emissionprob_ = np.array([[0.1,0.4,0.5],\n",
    "                               [0.6,0.3,0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving problem 1: (likelihood of a output sequence being seen, given is model)\n",
    "\n",
    "##### Forward Algorithm:\n",
    "* The probability of the first observation being “Walk” equals to the multiplication of the initial state distribution and emission probability matrix\n",
    "* The Forward Algorithm is a recursive algorithm for calculating αt(i) for the observation sequence of increasing length t . First, the probabilities for the single-symbol sequence are calculated as a product of initial i-th state probability and emission probability of the given symbol o(1) in the i-th state. Then the recursive formula is applied. Assume we have calculated αt(i) for some t. To calculate αt+1(j), we multiply every αt(i) by the corresponding transition probability from the i-th state to the j-th state, sum the products over all states, and then multiply the result by the emission probability of the symbol o(t+1). Iterating the process, we can eventually calculate αT(i), and then summing them over all states, we can obtain the required probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood of walk being happening is :0.30000000000000004\n",
      "Likelihood of shop being happening is :0.36000000000000004\n",
      "Likelihood of clean being happening is :0.3400000000000001\n",
      "Likelihood of walk-->shop-->clean being happening is :0.033611999999999996\n",
      "Likelihood of clean-->clean--->clean being happening is :0.04590400000000001\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"Likelihood of walk being happening is :{}\".format(math.exp(model.score(np.array([[0]])))))\n",
    "print(\"Likelihood of shop being happening is :{}\".format(math.exp(model.score(np.array([[1]])))))\n",
    "print(\"Likelihood of clean being happening is :{}\".format(math.exp(model.score(np.array([[2]])))))\n",
    "print(\"Likelihood of walk-->shop-->clean being happening is :{}\".format(math.exp(model.score(np.array([[0,1,2]])))))\n",
    "print(\"Likelihood of clean-->clean--->clean being happening is :{}\".format(math.exp(model.score(np.array([[2,2,2]])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving probelm 2: (Given model and output(emission) sequence, what is optimal hidden state sequence..?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of this being happening is :0.015120000000000003\n",
      "Sequence which would have generated this could be: [0 0 1]\n"
     ]
    }
   ],
   "source": [
    "logprob, seq= model.decode(np.array([[1,2,0]]).transpose())\n",
    "print(\"Probability of this being happening is :{}\".format(math.exp(logprob)))\n",
    "print(\"Sequence which would have generated this could be: {}\".format(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving Problem 3: \n",
    "##### The Viterbi algorithm \n",
    "\n",
    "* It chooses the best state sequence that maximizes the likelihood of the state sequence for the given observation sequence.\n",
    "* The observation made by the Viterbi algorithm is that for any state at time t, there is only one most likely path to that state. Therefore, if several paths converge at a particular state at time t, instead of recalculating them all when calculating the transitions from this state to states at time t+1, one can discard the less likely paths, and only use the most likely one in one's calculations. When this is applied to each time step, it greatly reduces the number of calculations to T*N^2, which is much nicer than N^T.\n",
    "\n",
    "http://blog.ivank.net/viterbi-algorithm-clarified.html <br>\n",
    "https://kastnerkyle.github.io/posts/single-speaker-word-recognition-with-hidden-markov-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"MC\"></a>\n",
    "\n",
    "### What is Monte Carlo...!\n",
    "\n",
    "* MCMC methods are used to approximate the posterior distribution of a parameter of interest by random sampling in a probabilistic space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Resources\"> </a>\n",
    "### Resources\n",
    "\n",
    "* https://brilliant.org/wiki/markov-chains/\n",
    "\n",
    "\n",
    "* Text processing example:\n",
    "\n",
    "  https://www.datasciencecentral.com/profiles/blogs/some-applications-of-markov-chain-in-python\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
